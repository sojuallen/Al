## find the data here: https://onedrive.live.com/?id=D1B0DAD863EFB7B5%211877&cid=D1B0DAD863EFB7B5

library(data.table)
library(Matrix)
library(dplyr)
library(MLmetrics)
library(lightgbm)
set.seed(257)

# Reading in data

train = fread("~/Allen Training/LightGBM/Lightgbm Python examples/train.csv") %>% as.data.frame()
test = fread("~/Allen Training/LightGBM/Lightgbm Python examples/test.csv") %>% as.data.frame()

# Pre Processing

median.impute <- function(x){
  x = as.data.frame(x)
  for (i in 1:ncol(x)){
    x[which(x[,i] == -1),i] = NA
  }
  x= x %>% mutate_all(~ifelse(is.na(.), median(., na.rm = TRUE), .)) %>% as.data.table()
  return(x)
  
}

train_pc <- median.impute(train)
test_pc<- median.impute(test)



# Feature Engineering

test_pc$target <- NA
m_data <- rbind(train_pc, test_pc)
m_data[, fe_amount_NA := rowSums(m_data == -1, na.rm = T)]
m_data[, ps_car_13_ps_reg_03 := ps_car_13*ps_reg_03]
m_data[, ps_reg_mult := ps_reg_01*ps_reg_02*ps_reg_03]



# Create LGB Dataset
varnames <- setdiff(colnames(m_data),c("id","target"))
train_sparse <- Matrix(as.matrix(m_data[!is.na(target),varnames, with= F]),sparse = TRUE)
test_sparse <- Matrix(as.matrix(m_data[is.na(target),varnames, with = F]), sparse = TRUE)

y_train <- m_data[!is.na(target),target]
test_ids <- m_data[is.na(target), id]

lgb.train <- lgb.Dataset(data = train_sparse, label = y_train)
categoricals.vec <- colnames(train_pc)[c(grep("cat",colnames(train)))]


# Setting up LGBM Parameters
lgb.grid <- list(objective = "binary",
                 metric = "auc",
                 min_sum_hessian_in_leaf = 1,
                 feature_fraction = 0.7,
                 bagging_fraction = 0.7,
                 baggin_freq = 5, 
                 min_data = 100, 
                 max_bin = 50,
                 lambda_l1 = 8,
                 lambda_l2 = 1.3,
                 min_data_in_bin = 100, 
                 min_gain_to_split = 10, 
                 min_data_in_leaf = 30, 
                 is_unbalance = TRUE)


# Setting up Gini Eval Function - Gini for Lgb
lgb.normalizedgini = function(preds, dtrain){
  actual = getinfo(dtrain, "label")
  score  = NormalizedGini(preds,actual)
  return(list(name = "gini", value = score, higher_better = TRUE))
}

# Cross Validataion
#lgb.model.cv = lgb.cv(params = lgb.grid, data = lgb.train, learning_rate = 0.02, num_leaves = 25,
#                   num_threads = 2 , nrounds = 7000, early_stopping_rounds = 50,
#                   eval_freq = 20, eval = lgb.normalizedgini,
#                   categorical_feature = categoricals.vec, nfold = 5, stratified = TRUE)

#best.iter = lgb.model.cv$best_iter
best.iter = 525
#Lgb has in built functions to create stratified k folds, which is pretty handy for our unbalanced dataset problem.


# Train Final Model

lgb.model <- lgb.train(params = lgb.grid, data = lgb.train, learning_rate = 0.02, 
                       num_leaves = 25, num_threads = 2, nrounds = best.iter,
                       eval_freq = 20, eval = lgb.normalizedgini, 
                       categorical_feature = categoricals.vec)


